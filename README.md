# ğŸ“– AskRAG - Chatbot

**AskRAG** is a simple Retrieval-Augmented Generation (RAG) chatbot built with **LangChain**, **Streamlit**, **Pinecone**, and **OpenAI**.
It demonstrates how to combine a vector database with a Large Language Model (LLM) to create a chatbot that can answer domain-specific questions with higher accuracy and reduced hallucination.

---

## ğŸš€ Features

* ğŸ”¹ **RAG Pipeline** â†’ retrieval + generation for more accurate answers
* ğŸ”¹ **Streamlit Frontend** â†’ clean, interactive web UI
* ğŸ”¹ **LangChain** â†’ orchestration of prompt, retriever, and LLM
* ğŸ”¹ **Pinecone** â†’ scalable vector storage & retrieval
* ğŸ”¹ **OpenAI API** â†’ embeddings + natural language understanding

---

## ğŸ› ï¸ Tech Stack

* **Frontend** â†’ Streamlit
* **Backend** â†’ LangChain
* **LLM** â†’ OpenAI GPT models
* **Vector Database** â†’ Pinecone

---

## ğŸ“‚ Project Structure

```
Ask-RAG/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ RAG_ChatBot.py        # Chatbot class (retrieval + generation pipeline)
â”‚   â”œâ”€â”€ streamlitMain.py      # Streamlit frontend
â”‚   â”œâ”€â”€ materials/            # Knowledge base (text/PDF files)
â”‚â”€â”€ .env                      # API keys (OpenAI & Pinecone)
â”‚â”€â”€ requirements.txt          # Dependencies
```

---

## ğŸ”‘ Environment Setup

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=your_pinecone_environment_here
```

---

## âš™ï¸ Installation

1. **Clone the repo**

   ```bash
   git clone https://github.com/itsaman080/Ask-RAG.git
   cd Ask-RAG
   ```

2. **Create virtual environment & install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

3. **Add data**
   Place your domain-specific documents (e.g., travel guides, policies) inside the `src/materials/` folder.

---

## â–¶ï¸ Running the Chatbot

Run the Streamlit app:

```bash
cd src
streamlit run streamlitMain.py
```

Open the provided URL (default: `http://localhost:8501`) to chat with **AskRAG**.

---

## ğŸ“Š How It Works

1. **Data Preparation** â†’ documents are chunked and converted into embeddings.
2. **Indexing** â†’ embeddings are stored in Pinecone.
3. **Retrieval** â†’ relevant chunks are fetched based on user queries.
4. **Prompt Engineering** â†’ retrieved context + query form the final LLM prompt.
5. **Generation** â†’ OpenAI LLM produces a context-aware answer.


---

## ğŸ“œ License

MIT License.
